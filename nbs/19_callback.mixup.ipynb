{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from fastai.basics import *\n",
    "from torch.distributions.beta import Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *\n",
    "from fastai.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MixUp and Friends\n",
    "\n",
    "> Callbacks that can apply the MixUp (and variants) data augmentation to your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def reduce_loss(\n",
    "    loss:Tensor, \n",
    "    reduction:str='mean' # PyTorch loss reduction\n",
    ")->Tensor:\n",
    "    \"Reduce the loss based on `reduction`\"\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MixHandler(Callback):\n",
    "    \"A handler class for implementing `MixUp` style scheduling\"\n",
    "    run_valid = False\n",
    "    def __init__(self, \n",
    "        alpha:float=0.5 # Determine `Beta` distribution in range (0.,inf]\n",
    "    ):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_train(self):\n",
    "        \"Determine whether to stack y\"\n",
    "        self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "        if self.stack_y: self.old_lf,self.learn.loss_func = self.learn.loss_func,self.lf\n",
    "\n",
    "    def after_train(self):\n",
    "        \"Set the loss function back to the previous loss\"\n",
    "        if self.stack_y: self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def after_cancel_train(self):\n",
    "        \"If training is canceled, still set the loss function back\"\n",
    "        self.after_train()\n",
    "\n",
    "    def after_cancel_fit(self):\n",
    "        \"If fit is canceled, still set the loss function back\"\n",
    "        self.after_train()\n",
    "\n",
    "    def lf(self, pred, *yb):\n",
    "        \"lf is a loss function that applies the original loss function on both outputs based on `self.lam`\"\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most `Mix` variants will perform the data augmentation on the batch, so to implement your `Mix` you should adjust the `before_batch` event with however your training regiment requires. Also if a different loss function is needed, you should adjust the `lf` as well. `alpha` is passed to `Beta` to create a sampler.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MixUp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MixUp(MixHandler):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412\"\n",
    "    def __init__(self, \n",
    "        alpha:float=.4 # Determine `Beta` distribution in range (0.,inf]\n",
    "    ): \n",
    "        super().__init__(alpha)\n",
    "        \n",
    "    def before_batch(self):\n",
    "        \"Blend xb and yb with another random item in a second batch (xb1,yb1) with `lam` weights\"\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n",
    "        xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle))\n",
    "        nx_dims = len(self.x.size())\n",
    "        self.learn.xb = tuple(L(xb1,self.xb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=nx_dims-1)))\n",
    "\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified implementation of mixup that will always blend at least 50% of the original image.  The original paper calls for a Beta distribution which is passed the same value of alpha for each position in the loss function (alpha = beta = #).  Unlike the original paper, this implementation of mixup selects the max of lambda which means that if the value that is sampled as lambda is less than 0.5 (i.e the original image would be <50% represented, 1-lambda is used instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blending of two images is determined by `alpha`.  \n",
    "\n",
    "$alpha=1.$:\n",
    "\n",
    "* All values between 0 and 1 have an equal chance of being sampled. \n",
    "* Any amount of mixing between the two images is possible  \n",
    "\n",
    "$alpha<1.$:\n",
    "\n",
    "* The values closer to 0 and 1 become more likely to be sampled than the values near 0.5.  \n",
    "* It is more likely that one of the images will be selected with a slight amount of the other image.  \n",
    "\n",
    "$alpha>1.$:\n",
    "\n",
    "* The values closer to 0.5 become more likely than the numbers close to 0 or 1.\n",
    "* It is more likely that the images will be blended evenly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at a very minimalistic example to show how our data is being generated with the `PETS` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "pat        = r'([^/]+)_\\d+.*$'\n",
    "fnames     = get_image_files(path/'images')\n",
    "item_tfms  = [Resize(256, method='crop')]\n",
    "batch_tfms = [*aug_transforms(size=224), Normalize.from_stats(*imagenet_stats)]\n",
    "dls = ImageDataLoaders.from_name_re(path, fnames, pat, bs=64, item_tfms=item_tfms, \n",
    "                                    batch_tfms=batch_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the results of our `Callback` by grabbing our data during `fit` at `before_batch` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = MixUp(1.)\n",
    "with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=mixup) as learn:\n",
    "    learn.epoch,learn.training = 0,True\n",
    "    learn.dl = dls.train\n",
    "    b = dls.one_batch()\n",
    "    learn._split(b)\n",
    "    learn('before_train')\n",
    "    learn('before_batch')\n",
    "\n",
    "_,axs = plt.subplots(3,3, figsize=(9,9))\n",
    "dls.show_batch(b=(mixup.x,mixup.y), ctxs=axs.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_ne(b[0], mixup.x)\n",
    "test_eq(b[1], mixup.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that every so often an image gets \"mixed\" with another. \n",
    "\n",
    "How do we train? You can pass the `Callback` either to `Learner` directly or to `cbs` in your fit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "learn = vision_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=[error_rate])\n",
    "learn.fit_one_cycle(1, cbs=mixup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CutMix(MixHandler):\n",
    "    \"Implementation of https://arxiv.org/abs/1905.04899\"\n",
    "    def __init__(self,\n",
    "        alpha:float=1. # Determine `Beta` distribution in range (0.,inf]\n",
    "    ):\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Add `rand_bbox` patches with size based on `lam` and location chosen randomly.\"\n",
    "        bs, _, H, W = self.x.size()\n",
    "        self.lam = self.distrib.sample((1,)).to(self.x.device)\n",
    "        shuffle = torch.randperm(bs).to(self.x.device)\n",
    "        xb1,self.yb1 = self.x[shuffle], tuple((self.y[shuffle],))\n",
    "        x1, y1, x2, y2 = self.rand_bbox(W, H, self.lam)\n",
    "        self.learn.xb[0][..., y1:y2, x1:x2] = xb1[..., y1:y2, x1:x2]\n",
    "        self.lam = (1 - ((x2-x1)*(y2-y1))/float(W*H))\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "\n",
    "    def rand_bbox(self,\n",
    "        W:int, # Input image width\n",
    "        H:int, # Input image height\n",
    "        lam:Tensor # lambda sample from Beta distribution i.e tensor([0.3647])\n",
    "    ) -> tuple: # Represents the top-left pixel location and the bottom-right pixel location\n",
    "        \"Give a bounding box location based on the size of the im and a weight\"\n",
    "        cut_rat = torch.sqrt(1. - lam).to(self.x.device)\n",
    "        cut_w = torch.round(W * cut_rat).type(torch.long).to(self.x.device)\n",
    "        cut_h = torch.round(H * cut_rat).type(torch.long).to(self.x.device)\n",
    "        # uniform\n",
    "        cx = torch.randint(0, W, (1,)).to(self.x.device)\n",
    "        cy = torch.randint(0, H, (1,)).to(self.x.device)\n",
    "        x1 = torch.clamp(cx - torch.div(cut_w, 2, rounding_mode='floor'), 0, W)\n",
    "        y1 = torch.clamp(cy - torch.div(cut_h, 2, rounding_mode='floor'), 0, H)\n",
    "        x2 = torch.clamp(cx + torch.div(cut_w, 2, rounding_mode='floor'), 0, W)\n",
    "        y2 = torch.clamp(cy + torch.div(cut_h, 2, rounding_mode='floor'), 0, H)\n",
    "        return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `MixUp`, `CutMix` will cut a random box out of two images and swap them together. We can look at a few examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix = CutMix(1.)\n",
    "with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=cutmix) as learn:\n",
    "    learn.epoch,learn.training = 0,True\n",
    "    learn.dl = dls.train\n",
    "    b = dls.one_batch()\n",
    "    learn._split(b)\n",
    "    learn('before_train')\n",
    "    learn('before_batch')\n",
    "\n",
    "_,axs = plt.subplots(3,3, figsize=(9,9))\n",
    "dls.show_batch(b=(cutmix.x,cutmix.y), ctxs=axs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train with it in the exact same way as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "learn = vision_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate])\n",
    "learn.fit_one_cycle(1, cbs=cutmix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
