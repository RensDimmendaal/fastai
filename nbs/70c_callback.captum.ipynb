{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "import tempfile\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captum\n",
    "Captum is the Model Interpretation Library from PyTorch as available [here](https://captum.ai)\n",
    "\n",
    "To use this we need to install the package using \n",
    "\n",
    "`conda install captum -c pytorch`\n",
    "\n",
    "or \n",
    "\n",
    "`pip install captum`\n",
    "\n",
    "This is a Call back to use Captum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from ipykernel import jsonutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Dirty hack as json_clean doesn't support CategoryMap type\n",
    "_json_clean=jsonutil.json_clean\n",
    "def json_clean(o):\n",
    "    o = list(o.items) if isinstance(o,CategoryMap) else o\n",
    "    return _json_clean(o)\n",
    "\n",
    "jsonutil.json_clean = json_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from captum.attr import IntegratedGradients,NoiseTunnel,GradientShap,Occlusion\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all this notebook, we will use the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'\n",
    "fnames = get_image_files(path)\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, fnames, valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captum Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distill Article [here](https://distill.pub/2020/attribution-baselines/) provides a good overview of what baseline image to choose. We can try them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CaptumInterpretation():\n",
    "    \"Captum Interpretation for Resnet\"\n",
    "    def __init__(self,learn,cmap_name='custom blue',colors=None,N=256,methods=('original_image','heat_map'),\n",
    "                 signs=(\"all\", \"positive\"),outlier_perc=1):\n",
    "        if colors is None: colors = [(0, '#ffffff'),(0.25, '#000000'),(1, '#000000')]\n",
    "        store_attr()\n",
    "        self.dls,self.model = learn.dls,self.learn.model\n",
    "        self.supported_metrics=['IG','NT','Occl']\n",
    "\n",
    "    def get_baseline_img(self, img_tensor,baseline_type):\n",
    "        baseline_img=None\n",
    "        if baseline_type=='zeros': baseline_img= img_tensor*0\n",
    "        if baseline_type=='uniform': baseline_img= torch.rand(img_tensor.shape)\n",
    "        if baseline_type=='gauss':\n",
    "            baseline_img= (torch.rand(img_tensor.shape).to(self.dls.device)+img_tensor)/2\n",
    "        return baseline_img.to(self.dls.device)\n",
    "\n",
    "    def visualize(self,inp,metric='IG',n_steps=1000,baseline_type='zeros',nt_type='smoothgrad', strides=(3,4,4), sliding_window_shapes=(3,15,15)):\n",
    "        if metric not in self.supported_metrics:\n",
    "            raise Exception(f\"Metric {metric} is not supported. Currently {self.supported_metrics} are only supported\")\n",
    "        tls = L([TfmdLists(inp, t) for t in L(ifnone(self.dls.tfms,[None]))])\n",
    "        inp_data=list(zip(*(tls[0],tls[1])))[0]\n",
    "        enc_data,dec_data=self._get_enc_dec_data(inp_data)\n",
    "        attributions=self._get_attributions(enc_data,metric,n_steps,nt_type,baseline_type,strides,sliding_window_shapes)\n",
    "        self._viz(attributions,dec_data,metric)\n",
    "\n",
    "    def _viz(self,attributions,dec_data,metric):\n",
    "        default_cmap = LinearSegmentedColormap.from_list(self.cmap_name,self.colors, N=self.N)\n",
    "        _ = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                              np.transpose(dec_data[0].numpy(), (1,2,0)),\n",
    "                                              methods=self.methods,\n",
    "                                              cmap=default_cmap,\n",
    "                                              show_colorbar=True,\n",
    "                                              signs=self.signs,\n",
    "                                              outlier_perc=self.outlier_perc, titles=[f'Original Image - ({dec_data[1]})', metric])\n",
    "\n",
    "\n",
    "\n",
    "    def _get_enc_dec_data(self,inp_data):\n",
    "        dec_data=self.dls.after_item(inp_data)\n",
    "        enc_data=self.dls.after_batch(to_device(self.dls.before_batch(dec_data),self.dls.device))\n",
    "        return(enc_data,dec_data)\n",
    "\n",
    "    def _get_attributions(self,enc_data,metric,n_steps,nt_type,baseline_type,strides,sliding_window_shapes):\n",
    "        # Get Baseline\n",
    "        baseline=self.get_baseline_img(enc_data[0],baseline_type)\n",
    "        supported_metrics ={}\n",
    "        if metric == 'IG':\n",
    "            self._int_grads = self._int_grads if hasattr(self,'_int_grads') else IntegratedGradients(self.model)\n",
    "            return self._int_grads.attribute(enc_data[0],baseline, target=enc_data[1], n_steps=200)\n",
    "        elif metric == 'NT':\n",
    "            self._int_grads = self._int_grads if hasattr(self,'_int_grads') else IntegratedGradients(self.model)\n",
    "            self._noise_tunnel= self._noise_tunnel if hasattr(self,'_noise_tunnel') else NoiseTunnel(self._int_grads)\n",
    "            return self._noise_tunnel.attribute(enc_data[0].to(self.dls.device), n_samples=1, nt_type=nt_type, target=enc_data[1])\n",
    "        elif metric == 'Occl':\n",
    "            self._occlusion = self._occlusion if hasattr(self,'_occlusion') else Occlusion(self.model)\n",
    "            return self._occlusion.attribute(enc_data[0].to(self.dls.device),\n",
    "                                       strides = strides,\n",
    "                                       target=enc_data[1],\n",
    "                                       sliding_window_shapes=sliding_window_shapes,\n",
    "                                       baselines=baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(CaptumInterpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum=CaptumInterpretation(learn)\n",
    "idx=randint(0,len(fnames))\n",
    "captum.visualize(fnames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum.visualize(fnames[idx],baseline_type='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum.visualize(fnames[idx],baseline_type='gauss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum.visualize(fnames[idx],metric='NT',baseline_type='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum.visualize(fnames[idx],metric='Occl',baseline_type='gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum Insights Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _formatted_data_iter(x: CaptumInterpretation,dl,normalize_func):\n",
    "    dl_iter=iter(dl)\n",
    "    while True:\n",
    "        images,labels=next(dl_iter)\n",
    "        images=normalize_func.decode(images).to(dl.device)\n",
    "        yield Batch(inputs=images, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def insights(x: CaptumInterpretation,inp_data,debug=True):\n",
    "    _baseline_func= lambda o: o*0\n",
    "    _get_vocab = lambda vocab: list(map(str,vocab)) if isinstance(vocab[0],bool) else vocab\n",
    "    dl = x.dls.test_dl(L(inp_data),with_labels=True, bs=4)\n",
    "    normalize_func= next((func for func in dl.after_batch if type(func)==Normalize),noop)\n",
    "\n",
    "    # captum v0.3 expects tensors without the batch dimension.\n",
    "    if nested_attr(normalize_func, 'mean.ndim', 4)==4: normalize_func.mean.squeeze_(0) \n",
    "    if nested_attr(normalize_func, 'std.ndim', 4)==4: normalize_func.std.squeeze_(0) \n",
    "\n",
    "    visualizer = AttributionVisualizer(\n",
    "        models=[x.model],\n",
    "        score_func=lambda o: torch.nn.functional.softmax(o, 1),\n",
    "        classes=_get_vocab(dl.vocab),\n",
    "        features=[ImageFeature(\"Image\", baseline_transforms=[_baseline_func], input_transforms=[normalize_func])],\n",
    "        dataset=x._formatted_data_iter(dl,normalize_func))\n",
    "    visualizer.render(debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum=CaptumInterpretation(learn)\n",
    "captum.insights(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
