{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current position (before `*args`) suggests it was designed to be usable as either a positional or keyword argument. This can be useful when:\n",
    "\n",
    "1. You want to maintain backwards compatibility with code that might be calling it positionally\n",
    "2. You expect it to be commonly used and want to allow shorter calls without keyword names\n",
    "3. The parameter has special significance that makes it logically part of the \"core\" arguments rather than optional kwargs\n",
    "\n",
    "However, given that it's causing parameter conflicts, and modern Python style generally favors explicit keyword arguments for clarity, moving it after `*args` (making it keyword-only) is probably the better design choice unless there's a specific need for positional usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp vision.core\n",
    "#|default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "try: BILINEAR,NEAREST = Image.Resampling.BILINEAR,Image.Resampling.NEAREST\n",
    "except AttributeError: from PIL.Image import BILINEAR,NEAREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = ['BILINEAR','NEAREST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = ['Image','ToTensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@patch\n",
    "def __repr__(x:Image.Image):\n",
    "    return \"<%s.%s image mode=%s size=%dx%d>\" % (x.__class__.__module__, x.__class__.__name__, x.mode, x.size[0], x.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core vision\n",
    "\n",
    "> Basic image opening/processing functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "cifar_stats    = ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\n",
    "mnist_stats    = ([0.131], [0.308])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(TEST_IMAGE).resize((30,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if not hasattr(Image,'_patched'):\n",
    "    _old_sz = Image.Image.size.fget\n",
    "    @patch(as_prop=True)\n",
    "    def size(x:Image.Image): return fastuple(_old_sz(x))\n",
    "    Image._patched = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def n_px(x: Image.Image): return x.size[0] * x.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.n_px, 30*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def shape(x: Image.Image): return x.size[1],x.size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.shape, (20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def aspect(x: Image.Image): return x.size[0]/x.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.aspect, 30/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def reshape(x: Image.Image, h, w, resample=0):\n",
    "    \"`resize` `x` to `(w,h)`\"\n",
    "    return x.resize((w,h), resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Image.Image.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.reshape(12,10).shape, (12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_bytes_format(im:Image.Image, format='png'):\n",
    "    \"Convert to bytes, default to PNG format\"\n",
    "    arr = io.BytesIO()\n",
    "    im.save(arr, format=format)\n",
    "    return arr.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Image.Image.to_bytes_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_thumb(self:Image.Image, h, w=None):\n",
    "    \"Same as `thumbnail`, but uses a copy\"\n",
    "    if w is None: w=h\n",
    "    im = self.copy()\n",
    "    im.thumbnail((w,h))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Image.Image.to_thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def resize_max(x: Image.Image, resample=0, max_px=None, max_h=None, max_w=None):\n",
    "    \"`resize` `x` to `max_px`, or `max_h`, or `max_w`\"\n",
    "    h,w = x.shape\n",
    "    if max_px and x.n_px>max_px: h,w = fastuple(h,w).mul(math.sqrt(max_px/x.n_px))\n",
    "    if max_h and h>max_h: h,w = (max_h    ,max_h*w/h)\n",
    "    if max_w and w>max_w: h,w = (max_w*h/w,max_w    )\n",
    "    return x.reshape(round(h), round(w), resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.resize_max(max_px=20*30).shape, (20,30))\n",
    "test_eq(im.resize_max(max_px=300).n_px, 294)\n",
    "test_eq(im.resize_max(max_px=500, max_h=10, max_w=20).shape, (10,15))\n",
    "test_eq(im.resize_max(max_h=14, max_w=15).shape, (10,15))\n",
    "test_eq(im.resize_max(max_px=300, max_h=10, max_w=25).shape, (10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Image.Image.resize_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section regroups the basic types used in vision with the transform that create objects of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_image(x):\n",
    "    \"Convert a tensor or array to a PIL int8 Image\"\n",
    "    if isinstance(x,Image.Image): return x\n",
    "    if isinstance(x,Tensor): x = to_np(x.permute((1,2,0)))\n",
    "    if x.dtype==np.float32: x = (x*255).astype(np.uint8)\n",
    "    return Image.fromarray(x, mode=['RGB','CMYK'][x.shape[0]==4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_image(fn, mode=None):\n",
    "    \"Open and load a `PIL.Image` and convert to `mode`\"\n",
    "    im = Image.open(fn)\n",
    "    im.load()\n",
    "    im = im._new(im.im)\n",
    "    return im.convert(mode) if mode else im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def image2tensor(img):\n",
    "    \"Transform image to byte tensor in `c*h*w` dim order.\"\n",
    "    res = tensor(img)\n",
    "    if res.dim()==2: res = res.unsqueeze(-1)\n",
    "    return res.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PILBase(Image.Image, metaclass=BypassNewMeta):\n",
    "    \"Base class for a Pillow `Image` that can show itself and convert to a Tensor\"\n",
    "    _bypass_type=Image.Image\n",
    "    _show_args = {'cmap':'viridis'}\n",
    "    _open_args = {'mode': 'RGB'}\n",
    "    @classmethod\n",
    "    def create(cls, fn:Path|str|Tensor|ndarray|bytes|Image.Image, **kwargs):\n",
    "        \"Return an Image from `fn`\"\n",
    "        if isinstance(fn,TensorImage): fn = fn.permute(1,2,0).type(torch.uint8)\n",
    "        if isinstance(fn,TensorMask): fn = fn.type(torch.uint8)\n",
    "        if isinstance(fn,Tensor): fn = fn.numpy()\n",
    "        if isinstance(fn,ndarray): return cls(Image.fromarray(fn))\n",
    "        if isinstance(fn,bytes): fn = io.BytesIO(fn)\n",
    "        if isinstance(fn,Image.Image): return cls(fn)\n",
    "        return cls(load_image(fn, **merge(cls._open_args, kwargs)))\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show image using `merge(self._show_args, kwargs)`\"\n",
    "        return show_image(self, ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "    def __repr__(self): return f'{self.__class__.__name__} mode={self.mode} size={\"x\".join([str(d) for d in self.size])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PILBase.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images passed to `PILBase` or inherited classes' `create` as a PyTorch `Tensor`, NumPy `ndarray`, or Pillow `Image` must already be in the correct Pillow image format. For example, `uint8`, and RGB or BW for `PILImage` or `PILImageBW`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PILBase.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PILImage(PILBase): \n",
    "    \"A RGB Pillow `Image` that can show itself and converts to `TensorImage`\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PILImageBW(PILImage):\n",
    "    \"A BW Pillow `Image` that can show itself and converts to `TensorImageBW`\"\n",
    "    _show_args,_open_args = {'cmap':'Greys'},{'mode': 'L'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PILImage.create(TEST_IMAGE)\n",
    "test_eq(type(im), PILImage)\n",
    "test_eq(im.mode, 'RGB')\n",
    "test_eq(str(im), 'PILImage mode=RGB size=1200x803')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2 = PILImage.create(im)\n",
    "test_eq(type(im2), PILImage)\n",
    "test_eq(im2.mode, 'RGB')\n",
    "test_eq(str(im2), 'PILImage mode=RGB size=1200x803')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.resize((64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = im.show(figsize=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timg = TensorImage(image2tensor(im))\n",
    "tpil = PILImage.create(timg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpil.resize((64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_eq(np.array(im), np.array(tpil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PILMask(PILBase):\n",
    "    \"A Pillow `Image` Mask that can show itself and converts to `TensorMask`\"\n",
    "    _open_args,_show_args = {'mode':'L'},{'alpha':0.5, 'cmap':'tab20'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PILMask.create(TEST_IMAGE)\n",
    "test_eq(type(im), PILMask)\n",
    "test_eq(im.mode, 'L')\n",
    "test_eq(str(im), 'PILMask mode=L size=1200x803')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "OpenMask = Transform(PILMask.create)\n",
    "OpenMask.loss_func = CrossEntropyLossFlat(axis=1)\n",
    "PILMask.create = OpenMask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = untar_data(URLs.MNIST_TINY)\n",
    "fns = get_image_files(mnist)\n",
    "mnist_fn = TEST_IMAGE_BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timg = Transform(PILImageBW.create)\n",
    "mnist_img = timg(mnist_fn)\n",
    "test_eq(mnist_img.size, (28,28))\n",
    "assert isinstance(mnist_img, PILImageBW)\n",
    "mnist_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AddMaskCodes(Transform):\n",
    "    \"Add the code metadata to a `TensorMask`\"\n",
    "    def __init__(self, codes=None):\n",
    "        self.codes = codes\n",
    "        if codes is not None: self.vocab,self.c = codes,len(codes)\n",
    "\n",
    "    def decodes(self, o:TensorMask):\n",
    "        if self.codes is not None: o.codes=self.codes\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid = untar_data(URLs.CAMVID_TINY)\n",
    "fns = get_image_files(camvid/'images')\n",
    "cam_fn = fns[0]\n",
    "mask_fn = camvid/'labels'/f'{cam_fn.stem}_P{cam_fn.suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img = PILImage.create(cam_fn)\n",
    "test_eq(cam_img.size, (128,96))\n",
    "tmask = Transform(PILMask.create)\n",
    "mask = tmask(mask_fn)\n",
    "test_eq(type(mask), PILMask)\n",
    "test_eq(mask.size, (128,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,3, figsize=(12,3))\n",
    "cam_img.show(ctx=axs[0], title='image')\n",
    "mask.show(alpha=1, ctx=axs[1], vmin=1, vmax=30, title='mask')\n",
    "cam_img.show(ctx=axs[2], title='superimposed')\n",
    "mask.show(ctx=axs[2], vmin=1, vmax=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorPoint(TensorBase):\n",
    "    \"Basic type for points in an image\"\n",
    "    _show_args = dict(s=10, marker='.', c='r')\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, t, img_size=None)->None:\n",
    "        \"Convert an array or a list of points `t` to a `Tensor`\"\n",
    "        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        if 'figsize' in kwargs: del kwargs['figsize']\n",
    "        x = self.view(-1,2)\n",
    "        ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "TensorPointCreate = Transform(TensorPoint.create)\n",
    "TensorPointCreate.loss_func = MSELossFlat()\n",
    "TensorPoint.create = TensorPointCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points are expected to come as an array/tensor of shape `(n,2)` or as a list of lists with two elements. Unless you change the defaults in `PointScaler` (see later on), coordinates should go from 0 to width/height, with the first one being the column index (so from 0 to width) and the second one being the row index (so from 0 to height).\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "This is different from the usual indexing convention for arrays in numpy or in PyTorch, but it's the way points are expected by matplotlib or the internal functions in PyTorch like `F.grid_sample`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnt_img = TensorImage(mnist_img.resize((28,35)))\n",
    "pnts = np.array([[0,0], [0,35], [28,0], [28,35], [9, 17]])\n",
    "tfm = Transform(TensorPoint.create)\n",
    "tpnts = tfm(pnts)\n",
    "test_eq(tpnts.shape, [5,2])\n",
    "test_eq(tpnts.dtype, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = pnt_img.show(figsize=(1,1), cmap='Greys')\n",
    "tpnts.show(ctx=ctx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_annotations(fname, prefix=None):\n",
    "    \"Open a COCO style json in `fname` and returns the lists of filenames (with maybe `prefix`) and labelled bboxes.\"\n",
    "    annot_dict = json.load(open(fname))\n",
    "    id2images, id2bboxes, id2cats = {}, collections.defaultdict(list), collections.defaultdict(list)\n",
    "    classes = {o['id']:o['name'] for o in annot_dict['categories']}\n",
    "    for o in annot_dict['annotations']:\n",
    "        bb = o['bbox']\n",
    "        id2bboxes[o['image_id']].append([bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]])\n",
    "        id2cats[o['image_id']].append(classes[o['category_id']])\n",
    "    id2images = {o['id']:ifnone(prefix, '') + o['file_name'] for o in annot_dict['images'] if o['id'] in id2bboxes}\n",
    "    ids = list(id2images.keys())\n",
    "    return [id2images[k] for k in ids], [(id2bboxes[k], id2cats[k]) for k in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `get_annotations` on the coco_tiny dataset against both image filenames and bounding box labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = untar_data(URLs.COCO_TINY)\n",
    "test_images, test_lbl_bbox = get_annotations(coco/'train.json')\n",
    "annotations = json.load(open(coco/'train.json'))\n",
    "categories, images, annots = map(lambda x:L(x),annotations.values())\n",
    "\n",
    "test_eq(test_images, images.attrgot('file_name'))\n",
    "\n",
    "def bbox_lbls(file_name):\n",
    "    img = images.filter(lambda img:img['file_name']==file_name)[0]\n",
    "    bbs = annots.filter(lambda a:a['image_id'] == img['id'])\n",
    "    i2o = {k['id']:k['name'] for k in categories}\n",
    "    lbls = [i2o[cat] for cat in bbs.attrgot('category_id')]\n",
    "    bboxes = [[bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]] for bb in bbs.attrgot('bbox')]\n",
    "    return [bboxes, lbls]\n",
    "\n",
    "for idx in random.sample(range(len(images)),5): \n",
    "    test_eq(test_lbl_bbox[idx], bbox_lbls(test_images[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from matplotlib import patches, patheffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def _draw_rect(ax, b, color='white', text=None, text_size=14, hw=True, rev=False):\n",
    "    lx,ly,w,h = b\n",
    "    if rev: lx,ly,w,h = ly,lx,h,w\n",
    "    if not hw: w,h = w-lx,h-ly\n",
    "    patch = ax.add_patch(patches.Rectangle((lx,ly), w, h, fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(lx,ly, text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBBox(TensorPoint):\n",
    "    \"Basic type for a tensor of bounding boxes in an image\"\n",
    "    @classmethod\n",
    "    def create(cls, x, img_size=None)->None: return cls(tensor(x).view(-1, 4).float(), img_size=img_size)\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        x = self.view(-1,4)\n",
    "        for b in x: _draw_rect(ctx, b, hw=False, **kwargs)\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding boxes are expected to come as tuple with an array/tensor of shape `(n,4)` or as a list of lists with four elements and a list of corresponding labels. Unless you change the defaults in `PointScaler` (see later on), coordinates for each bounding box should go from 0 to width/height, with the following convention: x1, y1, x2, y2 where (x1,y1) is your top-left corner and (x2,y2) is your bottom-right corner.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "We use the same convention as for points with x going from 0 to width and y going from 0 to height.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LabeledBBox(L):\n",
    "    \"Basic type for a list of bounding boxes in an image\"\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        for b,l in zip(self.bbox, self.lbl):\n",
    "            if l != '#na#': ctx = retain_type(b, self.bbox).show(ctx=ctx, text=l)\n",
    "        return ctx\n",
    "\n",
    "    bbox,lbl = add_props(lambda i,self: self[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = untar_data(URLs.COCO_TINY)\n",
    "images, lbl_bbox = get_annotations(coco/'train.json')\n",
    "idx=2\n",
    "coco_fn,bbox = coco/'train'/images[idx],lbl_bbox[idx]\n",
    "coco_img = timg(coco_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbbox = LabeledBBox(TensorBBox(bbox[0]), bbox[1])\n",
    "ctx = coco_img.show(figsize=(3,3), cmap='Greys')\n",
    "tbbox.show(ctx=ctx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless specifically mentioned, all the following transforms can be used as single-item transforms (in one of the list in the `tfms` you pass to a `TfmdDS` or a `Datasource`) or tuple transforms (in the `tuple_tfms` you pass to a `TfmdDS` or a `Datasource`). The safest way that will work across applications is to always use them as `tuple_tfms`. For instance, if you have points or bounding boxes as targets and use `Resize` as a single-item transform, when you get to `PointScaler` (which is a tuple transform) you won't have the correct size of the image to properly scale your points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "PILImage  ._tensor_cls = TensorImage\n",
    "PILImageBW._tensor_cls = TensorImageBW\n",
    "PILMask   ._tensor_cls = TensorMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@ToTensor\n",
    "def encodes(self, o:PILBase): return o._tensor_cls(image2tensor(o))\n",
    "@ToTensor\n",
    "def encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data augmentation transform that runs on PIL Images must be run before this transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ToTensor()\n",
    "print(tfm)\n",
    "print(type(mnist_img))\n",
    "print(type(tfm(mnist_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ToTensor()\n",
    "test_eq(tfm(mnist_img).shape, (1,28,28))\n",
    "test_eq(type(tfm(mnist_img)), TensorImageBW)\n",
    "test_eq(tfm(mask).shape, (96,128))\n",
    "test_eq(type(tfm(mask)), TensorMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we can pipeline this with `PILImage.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_img = Pipeline([PILImageBW.create, ToTensor()])\n",
    "img = pipe_img(mnist_fn)\n",
    "test_eq(type(img), TensorImageBW)\n",
    "pipe_img.show(img, figsize=(1,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cam_lbl(x): return mask_fn\n",
    "cam_tds = Datasets([cam_fn], [[PILImage.create, ToTensor()], [_cam_lbl, PILMask.create, ToTensor()]])\n",
    "show_at(cam_tds, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with data augmentation, and in particular the `grid_sample` method, points need to be represented with coordinates going from -1 to 1 (-1 being top or left, 1 bottom or right), which will be done unless you pass `do_scale=False`. We also need to make sure they are following our convention of points being x,y coordinates, so pass along `y_first=True` if you have your data in an y,x format to add a flip.\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "This transform needs to run on the tuple level, before any transform that changes the image size.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _scale_pnts(y, sz, do_scale=True, y_first=False):\n",
    "    if y_first: y = y.flip(1)\n",
    "    res = y * 2/tensor(sz).float() - 1 if do_scale else y\n",
    "    return TensorPoint(res, img_size=sz)\n",
    "\n",
    "def _unscale_pnts(y, sz): return TensorPoint((y+1) * tensor(sz).float()/2, img_size=sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PointScaler(Transform):\n",
    "    \"Scale a tensor representing points\"\n",
    "    order = 1\n",
    "    def __init__(self, do_scale=True, y_first=False): self.do_scale,self.y_first = do_scale,y_first\n",
    "    def _grab_sz(self, x):\n",
    "        self.sz = [x.shape[-1], x.shape[-2]] if isinstance(x, Tensor) else x.size\n",
    "        return x\n",
    "\n",
    "    def _get_sz(self, x): return getattr(x, 'img_size') if self.sz is None else self.sz\n",
    "\n",
    "    def setups(self, dl):\n",
    "        res = first(dl.do_item(None), risinstance(TensorPoint))\n",
    "        if res is not None: self.c = res.numel()\n",
    "\n",
    "    def encodes(self, x:PILBase|TensorImageBase): return self._grab_sz(x)\n",
    "    def decodes(self, x:PILBase|TensorImageBase): return self._grab_sz(x)\n",
    "\n",
    "    def encodes(self, x:TensorPoint): return _scale_pnts(x, self._get_sz(x), self.do_scale, self.y_first)\n",
    "    def decodes(self, x:TensorPoint): return _unscale_pnts(x.view(-1, 2), self._get_sz(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with data augmentation, and in particular the `grid_sample` method, points need to be represented with coordinates going from -1 to 1 (-1 being top or left, 1 bottom or right), which will be done unless you pass `do_scale=False`. We also need to make sure they are following our convention of points being x,y coordinates, so pass along `y_first=True` if you have your data in an y,x format to add a flip.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "This transform automatically grabs the sizes of the images it sees before a <code>TensorPoint</code> object and embeds it in them. For this to work, those images need to be before any points in the order of your final tuple. If you don't have such images, you need to embed the size of the corresponding image when creating a <code>TensorPoint</code> by passing it with `sz=...`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pnt_lbl(x): return TensorPoint.create(pnts)\n",
    "def _pnt_open(fn): return PILImage(PILImage.create(fn).resize((28,35)))\n",
    "pnt_tds = Datasets([mnist_fn], [_pnt_open, [_pnt_lbl]])\n",
    "pnt_tdl = TfmdDL(pnt_tds, bs=1, after_item=[PointScaler(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(pnt_tdl.after_item.c, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Check the size was grabbed by PointScaler and added to y\n",
    "tfm = PointScaler()\n",
    "tfm.as_item=False\n",
    "x,y = tfm(pnt_tds[0])\n",
    "test_eq(tfm.sz, x.size)\n",
    "test_eq(y.img_size, x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pnt_tdl.one_batch()\n",
    "#Scaling and flipping properly done\n",
    "#NB: we added a point earlier at (9,17); formula below scales to (-1,1) coords\n",
    "test_close(y[0], tensor([[-1., -1.], [-1.,  1.], [1.,  -1.], [1., 1.], [9/14-1, 17/17.5-1]]))\n",
    "a,b = pnt_tdl.decode_batch((x,y))[0]\n",
    "test_eq(b, tensor(pnts).float())\n",
    "#Check types\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorPoint)\n",
    "test_eq(type(a), TensorImage)\n",
    "test_eq(type(b), TensorPoint)\n",
    "test_eq(b.img_size, (28,35)) #Automatically picked the size of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnt_tdl.show_batch(figsize=(2,2), cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BBoxLabeler(Transform):\n",
    "    def setups(self, dl): self.vocab = dl.vocab\n",
    "\n",
    "    def decode (self, x, **kwargs):\n",
    "        self.bbox,self.lbls = None,None\n",
    "        return self._call('decodes', x, **kwargs)\n",
    "\n",
    "    def decodes(self, x:TensorMultiCategory):\n",
    "        self.lbls = [self.vocab[a] for a in x]\n",
    "        return x if self.bbox is None else LabeledBBox(self.bbox, self.lbls)\n",
    "\n",
    "    def decodes(self, x:TensorBBox):\n",
    "        self.bbox = x\n",
    "        return self.bbox if self.lbls is None else LabeledBBox(self.bbox, self.lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#LabeledBBox can be sent in a tl with MultiCategorize (depending on the order of the tls) but it is already decoded.\n",
    "@MultiCategorize\n",
    "def decodes(self, x:LabeledBBox): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@PointScaler\n",
    "def encodes(self, x:TensorBBox):\n",
    "    pnts = self.encodes(cast(x.view(-1,2), TensorPoint))\n",
    "    return cast(pnts.view(-1, 4), TensorBBox)\n",
    "\n",
    "@PointScaler\n",
    "def decodes(self, x:TensorBBox):\n",
    "    pnts = self.decodes(cast(x.view(-1,2), TensorPoint))\n",
    "    return cast(pnts.view(-1, 4), TensorBBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coco_bb(x):  return TensorBBox.create(bbox[0])\n",
    "def _coco_lbl(x): return bbox[1]\n",
    "\n",
    "coco_tds = Datasets([coco_fn], [PILImage.create, [_coco_bb], [_coco_lbl, MultiCategorize(add_na=True)]], n_inp=1)\n",
    "coco_tdl = TfmdDL(coco_tds, bs=1, after_item=[BBoxLabeler(), PointScaler(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Check the size was grabbed by PointScaler and added to y\n",
    "tfm = PointScaler()\n",
    "tfm.as_item=False\n",
    "x,y,z = tfm(coco_tds[0])\n",
    "test_eq(tfm.sz, x.size)\n",
    "test_eq(y.img_size, x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorize(add_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tds.tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = coco_tdl.one_batch()\n",
    "test_close(y[0], -1+tensor(bbox[0])/64)\n",
    "test_eq(z[0], tensor([1,1,1]))\n",
    "a,b,c = coco_tdl.decode_batch((x,y,z))[0]\n",
    "test_close(b, tensor(bbox[0]).float())\n",
    "test_eq(c.bbox, b)\n",
    "test_eq(c.lbl, bbox[1])\n",
    "\n",
    "#Check types\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorBBox)\n",
    "test_eq(type(z), TensorMultiCategory)\n",
    "test_eq(type(a), TensorImage)\n",
    "test_eq(type(b), TensorBBox)\n",
    "test_eq(type(c), LabeledBBox)\n",
    "test_eq(y.img_size, (128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tdl.show_batch();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#test other direction works too\n",
    "coco_tds = Datasets([coco_fn], [PILImage.create, [_coco_lbl, MultiCategorize(add_na=True)], [_coco_bb]])\n",
    "coco_tdl = TfmdDL(coco_tds, bs=1, after_item=[BBoxLabeler(), PointScaler(), ToTensor()])\n",
    "\n",
    "x,y,z = coco_tdl.one_batch()\n",
    "test_close(z[0], -1+tensor(bbox[0])/64)\n",
    "test_eq(y[0], tensor([1,1,1]))\n",
    "a,b,c = coco_tdl.decode_batch((x,y,z))[0]\n",
    "test_eq(b, bbox[1])\n",
    "test_close(c.bbox, tensor(bbox[0]).float())\n",
    "test_eq(c.lbl, b)\n",
    "\n",
    "#Check types\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorMultiCategory)\n",
    "test_eq(type(z), TensorBBox)\n",
    "test_eq(type(a), TensorImage)\n",
    "test_eq(type(b), MultiCategory)\n",
    "test_eq(type(c), LabeledBBox)\n",
    "test_eq(z.img_size, (128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
