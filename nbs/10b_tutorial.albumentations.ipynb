{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom transforms\n",
    "\n",
    "> Using `Datasets`, `Pipeline`, `TfmdLists` and `Transform` in computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own `Transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating your own `Transform` is way easier than you think. In fact, each time you have passed a label function to the data block API or to `ImageDataLoaders.from_name_func`, you have created a `Transform` without knowing it. At its base, a `Transform` is just a function. Let's show how you can easily add a transform by implementing one that wraps a data augmentation from the [albumentations library](https://github.com/albumentations-team/albumentations).\n",
    "\n",
    "First things first, you will need to install the albumentations library. Uncomment the following cell to do so if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it's going to be easier to see the result of the transform on a color image bigger than the mnist one we had before, so let's load something from the PETS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.PETS)\n",
    "items = get_image_files(source/\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still open it with `PILIlmage.create`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create(items[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how to wrap one transform, but you can as easily wrap any set of transforms you wrapped in a `Compose` method. Here let's do some `ShiftScaleRotate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ShiftScaleRotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The albumentations transform work on numpy images, so we just convert our `PILImage` to a numpy array before wrapping it back in `PILImage.create` (this function takes filenames as well as arrays or tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ShiftScaleRotate(p=1)\n",
    "def aug_tfm(img): \n",
    "    np_img = np.array(img)\n",
    "    aug_img = aug(image=np_img)['image']\n",
    "    return PILImage.create(aug_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tfm(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this function each time a `Transform` is expected and the fastai library will automatically do the conversion. That's because you can directly pass such a function to create a `Transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = Transform(aug_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some state in your transform, you might want to create a subclass of `Transform`. In that case, the function you want to apply should be written in the <code>encodes</code> method (the same way you implement `forward` for PyTorch module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform(Transform):\n",
    "    def __init__(self, aug): self.aug = aug\n",
    "    def encodes(self, img: PILImage):\n",
    "        aug_img = self.aug(image=np.array(img))['image']\n",
    "        return PILImage.create(aug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added a type annotation: this will make sure this transform is only applied to `PILImage`s and their subclasses. For any other object, it won't do anything. You can also write as many <code>encodes</code> method you want with different type-annotations and the `Transform` will properly dispatch the objects it receives.\n",
    "\n",
    "This is because in practice, the transform is often applied as an `item_tfms` (or a `batch_tfms`) that you pass in the data block API. Those items are a tuple of objects of different types, and the transform may have different behaviors on each part of the tuple.\n",
    "\n",
    "Let's check here how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = AlbumentationsTransform(ShiftScaleRotate(p=1))\n",
    "a,b = tfm((img, 'dog'))\n",
    "show_image(a, title=b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform was applied over the tuple `(img, \"dog\")`. `img` is a `PILImage`, so it applied the <code>encodes</code> method we wrote. `\"dog\"` is a string, so the transform did nothing to it. \n",
    "\n",
    "Sometimes however, you need your transform to take your tuple as whole: for instance albumentations is applied simultaneously on images and segmentation masks. In this case you need to subclass `ItemTransfrom` instead of `Transform`. Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_source = untar_data(URLs.CAMVID_TINY)\n",
    "cv_items = get_image_files(cv_source/'images')\n",
    "img = PILImage.create(cv_items[0])\n",
    "mask = PILMask.create(cv_source/'labels'/f'{cv_items[0].stem}_P{cv_items[0].suffix}')\n",
    "ax = img.show()\n",
    "ax = mask.show(ctx=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write a subclass of `ItemTransform` that can wrap any albumentations augmentation transform, but only for a segmentation problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationAlbumentationsTransform(ItemTransform):\n",
    "    def __init__(self, aug): self.aug = aug\n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "        aug = self.aug(image=np.array(img), mask=np.array(mask))\n",
    "        return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check how it gets applied on the tuple `(img, mask)`. This means you can pass it as an `item_tfms` in any segmentation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = SegmentationAlbumentationsTransform(ShiftScaleRotate(p=1))\n",
    "a,b = tfm((img, mask))\n",
    "ax = a.show()\n",
    "ax = b.show(ctx=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the same transforms in `after_item` but a different kind of targets (here segmentation masks), the targets are automatically processed as they should with the type-dispatch system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_source = untar_data(URLs.CAMVID_TINY)\n",
    "cv_items = get_image_files(cv_source/'images')\n",
    "cv_splitter = RandomSplitter(seed=42)\n",
    "cv_split = cv_splitter(cv_items)\n",
    "cv_label = lambda o: cv_source/'labels'/f'{o.stem}_P{o.suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageResizer(Transform):\n",
    "    order=1\n",
    "    \"Resize image to `size` using `resample`\"\n",
    "    def __init__(self, size, resample=BILINEAR):\n",
    "        if not is_listy(size): size=(size,size)\n",
    "        self.size,self.resample = (size[1],size[0]),resample\n",
    "\n",
    "    def encodes(self, o:PILImage): return o.resize(size=self.size, resample=self.resample)\n",
    "    def encodes(self, o:PILMask):  return o.resize(size=self.size, resample=NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create], [cv_label, PILMask.create]]\n",
    "cv_dsets = Datasets(cv_items, tfms, splits=cv_split)\n",
    "dls = cv_dsets.dataloaders(bs=64, after_item=[ImageResizer(128), ToTensor(), IntToFloatTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use the augmentation transform we created before, we just need to add one thing to it: we want it to be applied on the training set only, not the validation set. To do this, we specify it should only be applied on a specific `idx` of our splits by adding `split_idx=0` (0 is for the training set, 1 for the validation set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationAlbumentationsTransform(ItemTransform):\n",
    "    split_idx = 0\n",
    "    def __init__(self, aug): self.aug = aug\n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "        aug = self.aug(image=np.array(img), mask=np.array(mask))\n",
    "        return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check how it gets applied on the tuple `(img, mask)`. This means you can pass it as an `item_tfms` in any segmentation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dsets = Datasets(cv_items, tfms, splits=cv_split)\n",
    "dls = cv_dsets.dataloaders(bs=64, after_item=[ImageResizer(128), ToTensor(), IntToFloatTensor(), \n",
    "                                              SegmentationAlbumentationsTransform(ShiftScaleRotate(p=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different transform pipelines and the `DataBlock API`\n",
    "\n",
    "It's very common for us to use different transforms on the training dataset versus the validation dataset. Currently our `AlbumentationsTransform` will perform the same transform over both, let's see if we can make it a bit more flexible with what we want. \n",
    "\n",
    "Let's try to think of a scenario for our examle:\n",
    "\n",
    "I want to various data augmentations such as `HueSaturationValue` or `Flip` to operate similar to how fastai will do it, where they only run on the training dataset but not the validation dataset. What do we need to do to our `AlbumentationsTransform`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform(DisplayedTransform):\n",
    "    split_idx,order=0,2\n",
    "    def __init__(self, train_aug): store_attr()\n",
    "    \n",
    "    def encodes(self, img: PILImage):\n",
    "        aug_img = self.train_aug(image=np.array(img))['image']\n",
    "        return PILImage.create(aug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our newly written transform. But what changed?\n",
    "\n",
    "We added in a `split_idx`, which is what determines what transforms are run on the validation set and the training set (0 for train, 1 for validation, `None` for both).\n",
    "\n",
    "Along with this we set an `order` to `2`. What this entails is if we have any fastai transforms that perform a resize operation, those are done first before our new transform. This let's us know exactly when our transform is going to be applied, and how we can work with it!\n",
    "\n",
    "Let's look at an example with some `Composed` albumentations transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_aug(): return albumentations.Compose([\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our `ItemTransforms` with `Resize` and our new training augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tfms = [Resize(224), AlbumentationsTransform(get_train_aug())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the higher-level `DataBlock` API this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=item_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take a peek at some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.train.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our transforms were successfully only applied to our training data! Great!\n",
    "\n",
    "Now, what if we wanted special different behaviors applied to **both** the training and validation sets? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform(RandTransform):\n",
    "    \"A transform handler for multiple `Albumentation` transforms\"\n",
    "    split_idx,order=None,2\n",
    "    def __init__(self, train_aug, valid_aug): store_attr()\n",
    "    \n",
    "    def before_call(self, b, split_idx):\n",
    "        self.idx = split_idx\n",
    "    \n",
    "    def encodes(self, img: PILImage):\n",
    "        if self.idx == 0:\n",
    "            aug_img = self.train_aug(image=np.array(img))['image']\n",
    "        else:\n",
    "            aug_img = self.valid_aug(image=np.array(img))['image']\n",
    "        return PILImage.create(aug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's walk through what's happening here. We changed our `split_idx` to be `None`, which allows for us to say when we're setting our `split_idx`.\n",
    "\n",
    "We also inherit from `RandTransform`, which allows for us to set that `split_idx` in our `before_call`. \n",
    "\n",
    "Finally we check to see what the current `split_idx` *is*. If it's `0`, run the trainining augmentation, otherwise run the validation augmentation. \n",
    "\n",
    "Let's see an example of a typical training setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_aug(): return albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(224,224),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5)\n",
    "])\n",
    "\n",
    "def get_valid_aug(): return albumentations.Compose([\n",
    "    albumentations.CenterCrop(224,224, p=1.),\n",
    "    albumentations.Resize(224,224)\n",
    "], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll build our new `AlbumentationsTransform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tfms = [Resize(256), AlbumentationsTransform(get_train_aug(), get_valid_aug())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pass this into our `DataLoaders`:\n",
    "> Since we declared a resize already in our composed transforms, we do not need any item transforms present here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=item_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our training and validation augmentation again to find that they are indeed different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.train.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And looking at the shapes of the validation `DataLoader`'s `x`'s we'll find our `CenterCrop` was applied as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = dls.valid.one_batch()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "We used fastai's crop first as some padding is needed due to some image sizes being too small.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin -"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
